How do do diffuse lighting?

Parallelization scaling / consistency tests
new binary geometry format for storing facet body metadata
point-in-polyhedron optimization for stl body (https://wvannoordt.github.io/misc-math/unitary-cover.pdf), adapt using existing metadata
self-shadow check: draw tangent plane and check for light sources on the non-normal side?

Render a ball! Use to calibrate lighting

How to do lighting passses through transparent objects?

Metadata for scene object: I think this is where a huge amount of performance can be gained back.

Computing relevant bodies  should actually save some relevant metadata that can be used to revisit the computation later, I.e. what is the collision plane, what is the 
index of the collision face, etc.

The lighting structure needs some work. Specular and diffuse lighting need to be
treated differently. Specular lighting must take in a ray and output luminosity, 
whereas diffuse lighting generates its own ray. The color "pulling" will apply to
both cases (I think) and this is the approach that will be taken for now, but testing
needs to happen. Think about point light sources (light bulbs etc) interacting through
transparent media or between two objects... This will need a lot of thought.

Diffuse lighting also needs to be thought about to some extent. Simply drawing a ray to 
light sources will give uniform coloration of any surface facing a light. Maybe take dot
of the normal vector as a scaling factor? this would give uniform coloration on each face,
and then some small but finite reflectivity may make up for the rest... It might
also be worth adding some diffusivity parameter. 

It seems as though diffuse lighting is actually made up of three components: one component from a direct incidence trace, one from a normal trace, and one from a reflected (not specular!) trace.
Adding these three components MIGHT give an effect that approximates diffusifity, but who knows?

This brings up another point, the body-incidence check MUST be as efficient as possible since it is invoked so many times. It might be worth gridding every body in the scene that satisfies the 
quadrant check (this will probably need to be its own function now) and sorting in O(nlogn) by different sphereical axes. this would offer a very fast way (binary search) to check for incidence
using a quadrature on the sphere, without taking up too much extra memory. It will probably be a pain to implement but worth it in the long run.

Also, there are currently three issues happening:
	1: As each frame is rendered, render time increases.
	2: Sometimes, a frame will spontaneously take 5x longer to render. This may
		be related to the quadrant check, so some on-the-fly spatial computations might optimize that. Need to be careful about this one though.
	3: Memory usage is terrible. A simple render of 9 rectangular prisms used 1.2G of RAM.
		There is probably a "memory leak" (i.e., some optimization to be done) that
		causes accumulation of resources. This is absolutely worth optimizing.

If computing object metadata takes a few milliseconds whenever the camera / an object is moved, it will probably make for enormous savings later on when a million pixels are being computed.

Preliminary trace:
	get base color
	diffuse lighting viewing surface (gives the "solar" effect?)
	diffuse lighting impact surface (gives proper lighting)
	recursive:
		for reflected and refracted ray


scene-camera-object-bound-transform relationship?
Some fundamental points about this:
	A scene should take a ray as an input.
	Every ray defines a plane, and objects behind that plane cannot be seen.
	This can be extended to each dimension, not just the plane of the ray. Hence, bounding boxes are good!
	check as few objects as possible.
	Ray collision checks have multiple stages: first, the plane check stage (described above), (intermediate stage), then the bounding box (not for sphere, ellipsoid) and
		last-level checks (which both belong to the object).
	Ray tracing is recursive. On the most basic level, a ray enters into a function and a color comes out. there should be a limit on the depth (3?)
	refractions will probably need to compute the transmitting ray AND the exiting ray.
	Color influence decreases with ray depth (mirrors facing each other, exponential decay)
	
Is an interface / inherited class efficient? 

ANTIALIASING
	Antialiasing might have to be modularized so that each aspect of a scene performs antialiasing independently. a compute-chader stage could be applied as
	a global antialiasing. If each render also produces a map of distances to collisions, then shader stages can be applied later for general purposes too.
	Note also that this map may be generated a priori as an optimization, although some heavy hand calculations need to be carried out and throughly tested
	before that is possible
	
Parallel.For()
	This is not a good solution for speeding up rendering. This is akin to the "parfor" directive in matlab, where a compute pool is initialized and similar operations
	are performed on different datasets. Still searching for a good way to do more low-level parallelization... maybe just write this all in C++
	
Distance computation:
	This should probably be done in two stages: a rough pass that gets candidates based on overlap between intervals of maximal/minimal distance, then a fine pass that
	computes the distance exactly.
